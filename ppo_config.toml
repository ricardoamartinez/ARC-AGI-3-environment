# PPO/SAC configuration (loaded automatically by `main.py` if present).
# Env vars still override these values.

[trainer]
name = "rtac"                # online TD actor-critic update every step
action_mode = "vel"
require_goal = false
random_goal = true

[features]
mode = "somatic"             # ultra-light; uses explicit cursor/goal coords
dim = 64                     # small = faster per-step backprop
disable_lstm = true          # feedforward (cheapest), reduces UI lag

[logging]
log_every = 50
ui_fps = 60
ui_grid_fps = 5
ui_bayes_maps = false  # Disable slow Bayes heatmaps (saves ~3.7s per step!)

[reward]
# Dense continuing reward (no terminals). This is what the new RTAC trainer learns from.
mode = "dense"
scale = 2.0
goal_radius = 0.06
arrival_bonus = 2.0
action_l2_penalty = 0.001
vel_l2_penalty = 0.12
jerk_l2_penalty = 0.005

# Optional: extra wall penalty to avoid corner camping.
wall_penalty = 0.02

[physics]
# MAXIMUM SPEED cursor movement
bounds = 1.0
max_speed = 32.0              # MAXIMUM - cursor can move 32 cells/step
vel_response = 1.0
inertia_alpha = 1.0           # INSTANT response - no smoothing

# Physics params - INSTANT movement:
damping = 0.0                 # NO friction
pd_kp = 1.0                   # MAXIMUM acceleration
pd_kd = 0.0                   # NO damping - pure spring
stop_radius = 0.1             # almost no stickiness
stop_speed = 0.5              # high threshold
speed_scale_max = 10.0        # MAXIMUM speed scaling
target_ema_alpha = 1.0        # INSTANT target tracking - no smoothing
target_delta_max = 48.0       # can jump almost entire grid

[sac]
lr = 1e-3
gamma = 0.99
tau = 0.005
batch_size = 128
buffer_size = 10000
warmup = 200
updates_per_step = 4
auto_alpha = true
target_entropy = -0.2
features = "somatic" # fastest for this cursor-to-goal task
clear_buffer_on_goal_change = true
goal_change_burst_updates = 500
deterministic_after_steps = 500
async_updates = true
infer_device = "cpu"          # keeps UI smooth (no GPU contention)
sync_every_updates = 50       # how often learner syncs weights to the inference actor

[rtac]
lr = 3e-4
gamma = 0.97
td_lambda = 0.0
value_coef = 0.5
entropy_coef = 0.001
adv_clip = 3.0
max_grad_norm = 1.0
device = "cuda"              # learner device (fast backprop)
async_updates = true         # no-lag UI: learning runs in background
infer_device = "cpu"         # inference-only model for UI loop
sync_every_updates = 25      # tradeoff: lower = faster visible adaptation, higher = less overhead
queue_size = 512
torch_num_threads = 1
[world_model]
# V-JEPA 2 style world model - MAXIMUM SPEED
latent_dim = 64              # MINIMUM for speed
planning_horizon = 3         # SHORT planning
use_planner = true           # Enable CEM planning
use_curiosity = true         # Enable curiosity
curiosity_scale = 0.1        # Scale of curiosity reward
train_every = 64             # Train RARELY = maximum inference speed
exploration_steps = 200      # Start planning FAST
buffer_size = 5000           # Small buffer
batch_size = 16              # TINY batch = fastest training
lr = 1e-3                    # HIGH LR to compensate
